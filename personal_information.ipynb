{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同心云个人信息爬虫\n",
    "\n",
    "### 基于http://yun.tongji.edu.cn/addressbook/home?pageIndex= 提供的名单（含用户ID）分别在 http://yun.tongji.edu.cn/microblog/…/profile 和 http://yun.tongji.edu.cn/microblog/rest/microCard/getByUserId?userId=  获取个人[身份/学历]、[学院]、[专业]、[年级]、[邮箱]和[姓名]、[电话]。\n",
    "\n",
    "- 该版本运行后提供了可编辑的DataFrame [mydata] ,也同时实现了本地.csv保存功能。该DataFrame并不实时写出，而是只在读取完所有信息后写一次.csv，因而相当耗费内存。另一个即读即存的版本详见personal_information_pro。\n",
    "- 提供了文本形式的进度条。\n",
    "- 对于mydata可以调用如下参数（默认都是str格式）：\n",
    "\n",
    "字段参数 | 含义 | 备注 \n",
    "-|-|-\n",
    "degree | 在读学历 | [\"本科生\",\"研究生\",\"博士生\",\"教职工\"] |\n",
    "school | 学院 | / |\n",
    "major | 专业 | 对于身份为\"教职工\"的用户，该参数的值为\"\" |\n",
    "year | 年级 | / |\n",
    "mail | 邮箱 | 该参数可能缺省为\"\" |\n",
    "name | 姓名 | / |\n",
    "mobile | 电话 | 该参数可能缺省为\"\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as npy\n",
    "import re\n",
    "import json\n",
    "url = \"http://yun.tongji.edu.cn/space/c/rest/user/login\"\n",
    "UA = \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.13 Safari/537.36\"\n",
    " \n",
    "header = { \"User-Agent\" : UA,\n",
    "           \"Referer\": \"http://yun.tongji.edu.cn/public/login?supportNotMobile=false\"\n",
    "           }\n",
    " \n",
    "txy_session = requests.Session()\n",
    "f = txy_session.get(url,headers=header)\n",
    " \n",
    "myphone=input(\"同心云登陆手机号:\")\n",
    "psw=input(\"同心云登录密码:\")\n",
    "\n",
    "postData = {\n",
    "    'email':myphone,\n",
    "    'password': psw,\n",
    "    'remember':'true',\n",
    "    'accountType':'',\n",
    "    'accountName':'',\n",
    "    'networkId':'',\n",
    "    'redirectUrl':'', \n",
    "    'forceToNetwork':'false'\n",
    "}\n",
    " \n",
    "txy_session.post(url,\n",
    "                  data = postData,\n",
    "                  headers = header)\n",
    "\n",
    "# print(f.content.decode())\n",
    "\n",
    "\n",
    "\n",
    "pattern1=\"(?<=data-userid=\\\").*(?=\\\" data-username)\"  #在addressbook页面匹配所有data-userid后面接的内容\n",
    "pattern2=\"(?<=部门：\\n\t\t\t\t\t<span title=\\\").*(?=\\\">)\"  #在/profile页面匹配所有部门。注意，部门这个参数拆解后不定长，可能是4或5\n",
    "pattern3=\"(?<=\\n<li>邮箱：<a href=\\\"mailto:).*(?=\\\">)\"  #在/profile页面匹配所有邮箱。虽然长得很难看但却是能准确匹配到。\n",
    "\n",
    "degree=[]\n",
    "school=[]\n",
    "major=[]\n",
    "year=[]\n",
    "mail=[]\n",
    "user_name=[]\n",
    "mobile=[]\n",
    "mail2=[]\n",
    "\n",
    "lower_bound=input(\"输入页数下界（将包含下界）（min=1）\")\n",
    "upper_bound=input(\"输入页数上界 (将不含上界) (max=2913) \")\n",
    "filename=input(\"生成.csv的名字：\")\n",
    "num_of_pages=int(int(upper_bound)-int(lower_bound))\n",
    "page_count=-1\n",
    "print(\"将处理 \"+ str(num_of_pages) +\" 页共 \"+str(num_of_pages*20)+\" 条数据\\n\")\n",
    "for items in range(int(lower_bound),int(upper_bound)):\n",
    "# for items in range(1,3):\n",
    "    page_count+=1\n",
    "    src_1=\"http://yun.tongji.edu.cn/addressbook/home?pageIndex=\"+str(items)\n",
    "    print(src_1)\n",
    "    f = txy_session.get(src_1,headers=header)\n",
    "    soup = BeautifulSoup(f.content,\"html.parser\")\n",
    "    curr_content=str(soup.find('tbody'))\n",
    "    pt1=re.findall(pattern1,curr_content)\n",
    "    perc=0\n",
    "    for i in pt1:\n",
    "        perc+=5\n",
    "        print(\"\\rProcessing \"+str(perc)+\" % on \"+str(page_count)+\" of \"+str(num_of_pages)+\" pages...\",end=\" \")\n",
    "        #进入/profile页面(只含一个人的信息):\n",
    "        src_2 = \"http://yun.tongji.edu.cn/microblog/\"+ i +\"/profile\"\n",
    "        #print(src_2)\n",
    "        ff=txy_session.get(src_2,headers=header)\n",
    "        soup2 = BeautifulSoup(ff.content,\"html.parser\")\n",
    "        curr_content2=str(soup2.find('div',{'class':'bd c1'})) #缩小查找范围(可能没必要？)\n",
    "        #print(curr_content2)\n",
    "        #第一部分：获取专业和年级信息\n",
    "        pt2=re.findall(pattern2,curr_content2)\n",
    "        pt2=str(pt2)\n",
    "        curr_content3=pt2.split(\"&gt;\")\n",
    "        try:\n",
    "            #print(curr_content3[1])\n",
    "            degree.append(curr_content3[1])\n",
    "        except Exception as ee:\n",
    "            degree.append(\"\") # never happen\n",
    "        try:\n",
    "            #print(curr_content3[2])\n",
    "            school.append(curr_content3[2])\n",
    "        except Exception as ee:\n",
    "            school.append(\"\") # never happen\n",
    "        try:  #对于专业有必要做分类讨论，教职工没有该属性\n",
    "            if(len(curr_content3)>4):\n",
    "                #print(curr_content3[3])\n",
    "                major.append(curr_content3[3])\n",
    "            else:\n",
    "                #print(\"\")\n",
    "                major.append(\"\")\n",
    "        except Exception as ee:\n",
    "            major.append(\"\")  \n",
    "        try:\n",
    "            #print(curr_content3[len(curr_content3)-1][0:4])\n",
    "            year.append(curr_content3[len(curr_content3)-1][0:4])\n",
    "        except Exception as ee:\n",
    "            year.append(\"\") # last element alaways = year\n",
    "            \n",
    "        #第二部分：获取邮箱 (仍沿用curr_content2)/ 如果是qq邮箱解析出qq号\n",
    "        pt3=str(re.findall(pattern3,curr_content2))\n",
    "        if(pt3!=''):\n",
    "            pt3=pt3[2:len(pt3)-2]  #我也不太明白为什么原生数据会带一对双引号和方括号\n",
    "            mail.append(pt3)\n",
    "            #print(pt3)\n",
    "        else:\n",
    "            mail.append(\"\")\n",
    "            #print(\"\")\n",
    "        \n",
    "        #其实可以用下面部分的email参数来try的，一时没考虑到。\n",
    "        \n",
    "        #第三部分：获取个人姓名和手机号（一定成功 不用try）以及更多信息(需要try)\n",
    "        src_3=\"http://yun.tongji.edu.cn/microblog/rest/microCard/getByUserId?userId=\"+i\n",
    "        #print(src3)\n",
    "        fff=txy_session.get(src_3,headers=header)\n",
    "        soup3 = BeautifulSoup(fff.content,\"html.parser\")\n",
    "        s = json.loads(str(soup3))           \n",
    "        user_name.append(str(s['microCard']['name']))\n",
    "        try:\n",
    "            mobile.append(str(s['microCard']['mobiles'][0]))\n",
    "        except Exception as ee:\n",
    "            mobile.append(\"\")\n",
    "#         try:  \n",
    "#             mail2.append(str(s['microCard']['email']))\n",
    "#         except:\n",
    "#             mail2.append(\"\")\n",
    "\n",
    "\n",
    "summary={\"name\":user_name,\"degree\":degree,\"school\":school,\"major\":major,\"year\":year,\"mail\":mail,\"mobile\":mobile}\n",
    "mydata=pd.DataFrame(summary)\n",
    "print(\"\\r完成！                                                \",end=\" \")\n",
    "mydata.to_csv(filename+\".csv\", mode='a', index=False, sep=',', header=False, encoding=\"utf_8_sig\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
